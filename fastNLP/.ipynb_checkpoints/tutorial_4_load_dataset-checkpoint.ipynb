{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用Loader和Pipe加载并处理数据集\n",
    "\n",
    "这一部分是关于如何加载数据集的教程\n",
    "\n",
    "## Part I: 数据集容器DataBundle\n",
    "\n",
    "而由于对于同一个任务，训练集，验证集和测试集会共用同一个词表以及具有相同的目标值，所以在fastNLP中我们使用了 DataBundle 来承载同一个任务的多个数据集 DataSet 以及它们的词表 Vocabulary 。下面会有例子介绍 DataBundle 的相关使用。\n",
    "\n",
    "DataBundle 在fastNLP中主要在各个 Loader 和 Pipe 中被使用。 下面我们先介绍一下 Loader 和 Pipe 。\n",
    "\n",
    "## Part II: 加载的各种数据集的Loader\n",
    "\n",
    "在fastNLP中，所有的 Loader 都可以通过其文档判断其支持读取的数据格式，以及读取之后返回的 DataSet 的格式, 例如 ChnSentiCorpLoader \n",
    "\n",
    "- download() 函数：自动将该数据集下载到缓存地址，默认缓存地址为~/.fastNLP/datasets/。由于版权等原因，不是所有的Loader都实现了该方法。该方法会返回下载后文件所处的缓存地址。\n",
    "\n",
    "- _load() 函数：从一个数据文件中读取数据，返回一个 DataSet 。返回的DataSet的格式可从Loader文档判断。\n",
    "\n",
    "- load() 函数：从文件或者文件夹中读取数据为 DataSet 并将它们组装成 DataBundle。支持接受的参数类型有以下的几种\n",
    "\n",
    "    - None, 将尝试读取自动缓存的数据，仅支持提供了自动下载数据的Loader\n",
    "    - 文件夹路径, 默认将尝试在该文件夹下匹配文件名中含有 train , test , dev 的文件，如果有多个文件含有相同的关键字，将无法通过该方式读取\n",
    "    - dict, 例如{'train':\"/path/to/tr.conll\", 'dev':\"/to/validate.conll\", \"test\":\"/to/te.conll\"}。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In total 3 datasets:\n",
      "\tdev has 1914 instances.\n",
      "\ttrain has 17140 instances.\n",
      "\ttest has 1944 instances.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fastNLP.io import CWSLoader\n",
    "\n",
    "loader = CWSLoader(dataset_name='pku')\n",
    "data_bundle = loader.load()\n",
    "print(data_bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 49.2k/188k [00:00<00:00, 443kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://212.129.155.247/dataset/weibo_NER.zip not found in cache, downloading to /var/folders/1t/r3byrg0s5cb80qkz17dn8nc40000gn/T/tmp09zant2o\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188k/188k [00:00<00:00, 644kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish download from http://212.129.155.247/dataset/weibo_NER.zip\n",
      "Copy file to /Users/mac/.fastNLP/dataset/weibo_NER\n",
      "In total 3 datasets:\n",
      "\tdev has 270 instances.\n",
      "\ttest has 270 instances.\n",
      "\ttrain has 1350 instances.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fastNLP.io import WeiboNERLoader\n",
    "loader = WeiboNERLoader()\n",
    "loader.download()\n",
    "data_bundle = loader.load()\n",
    "print(data_bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+-------------------------------------------------+-------------------------------------------------+\n",
       "| raw_chars                                       | target                                          |\n",
       "+-------------------------------------------------+-------------------------------------------------+\n",
       "| ['科', '技', '全', '方', '位', '资', '讯', '... | ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'... |\n",
       "| ['对', '，', '输', '给', '一', '个', '女', '... | ['O', 'O', 'O', 'O', 'O', 'O', 'B-PER.NOM', ... |\n",
       "| ['今', '天', '下', '午', '起', '来', '看', '... | ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'... |\n",
       "| ['今', '年', '拜', '年', '不', '短', '信', '... | ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'... |\n",
       "| ['浑', '身', '酸', '疼', '，', '两', '腿', '... | ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'... |\n",
       "+-------------------------------------------------+-------------------------------------------------+"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_data = data_bundle.get_dataset(\"train\")\n",
    "tr_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 81.9k/3.80M [00:00<00:06, 552kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://212.129.155.247/dataset/MSRA_NER.zip not found in cache, downloading to /var/folders/1t/r3byrg0s5cb80qkz17dn8nc40000gn/T/tmpdwaounk6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3.80M/3.80M [00:05<00:00, 665kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish download from http://212.129.155.247/dataset/MSRA_NER.zip\n",
      "Copy file to /Users/mac/.fastNLP/dataset/MSRA_NER\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "+-------------------------------------------------+-------------------------------------------------+\n",
       "| raw_chars                                       | target                                          |\n",
       "+-------------------------------------------------+-------------------------------------------------+\n",
       "| ['当', '希', '望', '工', '程', '救', '助', '... | ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'... |\n",
       "| ['藏', '书', '本', '来', '就', '是', '所', '... | ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'... |\n",
       "| ['因', '有', '关', '日', '寇', '在', '京', '... | ['O', 'O', 'O', 'B-LOC', 'O', 'O', 'B-LOC', ... |\n",
       "| ['以', '家', '乡', '的', '历', '史', '文', '... | ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'... |\n",
       "| ['我', '们', '是', '受', '到', '郑', '振', '... | ['O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', ... |\n",
       "+-------------------------------------------------+-------------------------------------------------+"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastNLP.io import MsraNERLoader\n",
    "loader = MsraNERLoader()\n",
    "data_bundle = loader.load()\n",
    "tr_data = data_bundle.get_dataset(\"train\")\n",
    "tr_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Ontonotes cannot be downloaded automatically, you can refer https://github.com/yhcc/OntoNotes-5.0-NER to download and preprocess.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-4029ac90e3fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfastNLP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOntoNotesNERLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOntoNotesNERLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata_bundle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtr_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_bundle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtr_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/torch/lib/python3.7/site-packages/fastNLP/io/loader/loader.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, paths)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \"\"\"\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpaths\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_loader_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/torch/lib/python3.7/site-packages/fastNLP/io/loader/conll.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         raise RuntimeError(\"Ontonotes cannot be downloaded automatically, you can refer \"\n\u001b[0m\u001b[1;32m    286\u001b[0m                            \"https://github.com/yhcc/OntoNotes-5.0-NER to download and preprocess.\")\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Ontonotes cannot be downloaded automatically, you can refer https://github.com/yhcc/OntoNotes-5.0-NER to download and preprocess."
     ]
    }
   ],
   "source": [
    "from fastNLP.io import OntoNotesNERLoader\n",
    "loader = OntoNotesNERLoader()\n",
    "data_bundle = loader.load()\n",
    "tr_data = data_bundle.get_dataset(\"train\")\n",
    "tr_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里表示一共有3个数据集。其中：\n",
    "\n",
    "    3个数据集的名称分别为train、dev、test，分别有17223、1831、1944个instance\n",
    "\n",
    "也可以取出DataSet，并打印DataSet中的具体内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------+\n",
      "| raw_words                                                                            |\n",
      "+--------------------------------------------------------------------------------------+\n",
      "| 迈向  充满  希望  的  新  世纪  ——  一九九八年  新年  讲话  （  附  图片  １  张  ） |\n",
      "| （  一九九七年  十二月  三十一日  ）                                                 |\n",
      "+--------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "tr_data = data_bundle.get_dataset('train')\n",
    "print(tr_data[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+---------------------------------------------------------------------------------------------+\n",
       "| raw_words                                                                                   |\n",
       "+---------------------------------------------------------------------------------------------+\n",
       "| 迈向  充满  希望  的  新  世纪  ——  一九九八年  新年  讲话  （  附  图片  １  张  ）        |\n",
       "| （  一九九七年  十二月  三十一日  ）                                                        |\n",
       "| １２月  ３１日  ，  中共中央  总书记  、  国家  主席  江  泽民  发表  １９９８年  新年 ...  |\n",
       "| 在  １９９８年  来临  之际  ，  我  十分  高兴  地  通过  中央  人民  广播  电台  、  中... |\n",
       "| １９９７年  ，  是  中国  发展  历史  上  非常  重要  的  很  不  平凡  的  一  年  。 ...  |\n",
       "| 在  这  一  年  中  ，  中国  的  改革  开放  和  现代化  建设  继续  向前  迈进  。  国... |\n",
       "| 在  这  一  年  中  ，  中国  的  外交  工作  取得  了  重要  成果  。  通过  高层  互访... |\n",
       "| １９９８年  ，  中国  人民  将  满怀信心  地  开创  新  的  业绩  。  尽管  我们  在  经... |\n",
       "| 实现  祖国  的  完全  统一  ，  是  海内外  全体  中国  人  的  共同  心愿  。  通过  中... |\n",
       "| 台湾  是  中国  领土  不可分割  的  一  部分  。  完成  祖国  统一  ，  是  大势所趋  ，... |\n",
       "| 环顾  全球  ，  日益  密切  的  世界  经济  联系  ，  日新月异  的  科技  进步  ，  正在... |\n",
       "| 中国  政府  将  继续  坚持  奉行  独立自主  的  和平  外交  政策  ，  在  和平共处  五 ...  |\n",
       "| 在  这  辞旧迎新  的  美好  时刻  ，  我  祝  大家  新年  快乐  ，  家庭  幸福  ！          |\n",
       "| 谢谢  ！  （  新华社  北京  １２月  ３１日  电  ）                                          |\n",
       "| 在  十五大  精神  指引  下  胜利  前进  ——  元旦  献辞                                      |\n",
       "| 我们  即将  以  丰收  的  喜悦  送  走  牛年  ，  以  昂扬  的  斗志  迎来  虎年  。  我... |\n",
       "| 刚刚  过去  的  一  年  ，  大气磅礴  ，  波澜壮阔  。  在  这  一  年  ，  以  江  泽民... |\n",
       "| １９９８年  ，  是  全面  贯彻  落实  党  的  十五大  提  出  的  任务  的  第一  年  ，... |\n",
       "| 今年  是  党  的  十一  届  三中全会  召开  ２０  周年  ，  是  我们  党  和  国家  实现... |\n",
       "| 我们  要  更  好  地  坚持  解放思想  、  实事求是  的  思想  路线  。  解放思想  、  实... |\n",
       "| 我们  要  更  好  地  坚持  以  经济  建设  为  中心  。  各项  工作  必须  以  经济  建... |\n",
       "| 我们  要  更  好  地  坚持  “  两手抓  、  两手  都  要  硬  ”  的  方针  。  在  坚持...   |\n",
       "| 我们  要  更  好  地  发扬  求真务实  、  密切  联系  群众  的  作风  。  这  是  把  党... |\n",
       "| １９９８  ，  瞩目  中华  。  新  的  机遇  和  挑战  ，  催  人  进取  ；  新  的  目标... |\n",
       "| 北京  举行  新年  音乐会                                                                    |\n",
       "| 江  泽民  李  鹏  乔  石  朱  镕基  李  瑞环  刘  华清  尉  健行  李  岚清  与  万  名 ...  |\n",
       "| 党  和  国家  领导人  江  泽民  、  李  鹏  、  乔  石  、  朱  镕基  、  李  瑞环  、 ...  |\n",
       "| （  新华社  记者  樊  如钧  摄  ）                                                          |\n",
       "| 本报  北京  １２月  ３１日  讯  新华社  记者  陈  雁  、  本报  记者  何  加正  报道  ：... |\n",
       "| 今晚  的  长安街  流光溢彩  ，  火树银花  ；  人民  大会堂  里  灯火辉煌  ，  充满  欢乐... |\n",
       "| 音乐会  在  雄壮  的  管弦乐  《  红旗  颂  》  中  拉开  帷幕  ，  舒展  、  优美  的 ...  |\n",
       "| 万  人  大会堂  今晚  座无虚席  ，  观众  被  艺术家  们  精湛  的  表演  深深  打动  ，... |\n",
       "| 演出  结束  后  ，  江  泽民  等  党  和  国家  领导人  走  上  舞台  ，  亲切  会见  了... |\n",
       "| 李  鹏  在  北京  考察  企业                                                                |\n",
       "| 向  广大  职工  祝贺  新年  ，  对  节日  坚守  岗位  的  同志  们  表示  慰问              |\n",
       "| 上午  九时  二十分  ，  李  鹏  总理  在  北京  市委  书记  、  市长  贾  庆林  的  陪同... |\n",
       "| 在  总厂  所  属  的  石景山  热电厂  ，  李  鹏  首先  向  华北  电管局  、  电厂  负责... |\n",
       "| （  Ａ  、  Ｂ  ）                                                                          |\n",
       "| 李  鹏  说  ：  “  作为  首都  的  电力  工作者  ，  你们  为  首都  的  各项  重大  活...  |\n",
       "| ...                                                                                         |\n",
       "+---------------------------------------------------------------------------------------------+"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_bundle.get_dataset(\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: 使用Pipe对数据集进行预处理\n",
    "\n",
    "通过 Loader 可以将文本数据读入，但并不能直接被神经网络使用，还需要进行一定的预处理。\n",
    "\n",
    "在fastNLP中，我们使用 Pipe 的子类作为数据预处理的类， Loader 和 Pipe 一般具备一一对应的关系，该关系可以从其名称判断， 例如 CWSLoader 与 CWSPipe 是一一对应的。一般情况下Pipe处理包含以下的几个过程，\n",
    "1. 将raw_words或 raw_chars进行tokenize以切分成不同的词或字; \n",
    "2. 再建立词或字的 Vocabulary , 并将词或字转换为index; \n",
    "3. 将target 列建立词表并将target列转为index;\n",
    "\n",
    "所有的Pipe都可通过其文档查看该Pipe支持处理的 DataSet 以及返回的 DataBundle 中的Vocabulary的情况; 如 OntoNotesNERPipe\n",
    "\n",
    "各种数据集的Pipe当中，都包含了以下的两个函数:\n",
    "\n",
    "- process() 函数：对输入的 DataBundle 进行处理, 然后返回处理之后的 DataBundle 。process函数的文档中包含了该Pipe支持处理的DataSet的格式。\n",
    "- process_from_file() 函数：输入数据集所在文件夹，使用对应的Loader读取数据(所以该函数支持的参数类型是由于其对应的Loader的load函数决定的)，然后调用相对应的process函数对数据进行预处理。相当于是把Load和process放在一个函数中执行。\n",
    "\n",
    "接着上面 CWSLoader 的例子，我们展示一下 CWSPipe 的功能："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In total 3 datasets:\n",
      "\tdev has 1914 instances.\n",
      "\ttrain has 17140 instances.\n",
      "\ttest has 1944 instances.\n",
      "In total 2 vocabs:\n",
      "\tchars has 4777 entries.\n",
      "\ttarget has 4 entries.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fastNLP.io import CWSPipe\n",
    "\n",
    "data_bundle = CWSPipe().process(data_bundle)\n",
    "print(data_bundle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "表示一共有3个数据集和2个词表。其中：\n",
    "\n",
    "- 3个数据集的名称分别为train、dev、test，分别有17223、1831、1944个instance\n",
    "- 2个词表分别为chars词表与target词表。其中chars词表为句子文本所构建的词表，一共有4777个不同的字；target词表为目标标签所构建的词表，一共有4种标签。\n",
    "\n",
    "相较于之前CWSLoader读取的DataBundle，新增了两个Vocabulary。 我们可以打印一下处理之后的DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+---------------------------+---------------------------+---------+\n",
      "| raw_words                 | chars                     | target                    | seq_len |\n",
      "+---------------------------+---------------------------+---------------------------+---------+\n",
      "| 迈向  充满  希望  的  ... | [1224, 178, 674, 544, ... | [0, 1, 0, 1, 0, 1, 2, ... | 29      |\n",
      "| （  一九九七年  十二月... | [65, 9, 483, 483, 746,... | [2, 0, 3, 3, 3, 1, 0, ... | 14      |\n",
      "+---------------------------+---------------------------+---------------------------+---------+\n"
     ]
    }
   ],
   "source": [
    "tr_data = data_bundle.get_dataset('train')\n",
    "print(tr_data[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ws_to_cs(instance):\n",
    "    cs = []\n",
    "    ws = instance[\"raw_words\"]\n",
    "    for w in ws.split():\n",
    "        for c in w:\n",
    "            cs.append(c)\n",
    "    return cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+-----------------------+-----------------------+-----------------------+---------+-----------------------+\n",
       "| raw_words             | chars                 | target                | seq_len | raw_chars             |\n",
       "+-----------------------+-----------------------+-----------------------+---------+-----------------------+\n",
       "| 迈向  充满  希望  ... | [1224, 178, 674, 5... | [0, 1, 0, 1, 0, 1,... | 29      | ['迈', '向', '充',... |\n",
       "| （  一九九七年  十... | [65, 9, 483, 483, ... | [2, 0, 3, 3, 3, 1,... | 14      | ['（', '一', '九',... |\n",
       "+-----------------------+-----------------------+-----------------------+---------+-----------------------+"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastNLP import Vocabulary\n",
    "\n",
    "tr_data.apply(ws_to_cs, new_field_name=\"raw_chars\")\n",
    "tr_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocabulary(['迈', '向', '充', '满', '希']...)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastNLP import Vocabulary\n",
    "\n",
    "vocab = Vocabulary()\n",
    "vocab.from_dataset(tr_data, field_name=\"raw_chars\")\n",
    "vocab.index_dataset(tr_data, field_name=\"raw_chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+-----------------------+-----------------------+-----------------------+---------+-----------------------+\n",
       "| raw_words             | chars                 | target                | seq_len | raw_chars             |\n",
       "+-----------------------+-----------------------+-----------------------+---------+-----------------------+\n",
       "| 迈向  充满  希望  ... | [1224, 178, 674, 5... | [0, 1, 0, 1, 0, 1,... | 29      | [1265, 192, 664, 5... |\n",
       "| （  一九九七年  十... | [65, 9, 483, 483, ... | [2, 0, 3, 3, 3, 1,... | 14      | [65, 7, 490, 490, ... |\n",
       "| １２月  ３１日  ，... | [6, 5, 72, 6, 5, 3... | [0, 3, 1, 0, 3, 1,... | 55      | [11, 45, 75, 87, 1... |\n",
       "| 在  １９９８年  来... | [10, 6, 5, 16, 42,... | [2, 0, 3, 1, 0, 1,... | 97      | [8, 11, 30, 30, 12... |\n",
       "| １９９７年  ，  是... | [6, 5, 16, 2, 15, ... | [0, 3, 1, 2, 2, 0,... | 177     | [11, 30, 30, 120, ... |\n",
       "+-----------------------+-----------------------+-----------------------+---------+-----------------------+"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到有两列为int的field: chars和target。这两列的名称同时也是DataBundle中的Vocabulary的名称。可以通过下列的代码获取并查看Vocabulary的 信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary(['B', 'E', 'S', 'M']...)\n"
     ]
    }
   ],
   "source": [
    "vocab = data_bundle.get_vocab('target')\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IV: fastNLP封装好的Loader和Pipe\n",
    "\n",
    "fastNLP封装了多种任务/数据集的 Loader 和 Pipe 并提供自动下载功能，具体参见文档 [数据集](https://docs.qq.com/sheet/DVnpkTnF6VW9UeXdh?c=A1A0A0)\n",
    "\n",
    "## Part V: 不同格式类型的基础Loader\n",
    "\n",
    "除了上面提到的针对具体任务的Loader，我们还提供了CSV格式和JSON格式的Loader\n",
    "\n",
    "**CSVLoader** 读取CSV类型的数据集文件。例子如下：\n",
    "\n",
    "```python\n",
    "from fastNLP.io.loader import CSVLoader\n",
    "data_set_loader = CSVLoader(\n",
    "    headers=('raw_words', 'target'), sep='\\t'\n",
    ")\n",
    "```\n",
    "\n",
    "表示将CSV文件中每一行的第一项将填入'raw_words' field，第二项填入'target' field。其中项之间由'\\t'分割开来\n",
    "\n",
    "```python\n",
    "data_set = data_set_loader._load('path/to/your/file')\n",
    "```\n",
    "\n",
    "文件内容样例如下\n",
    "\n",
    "```csv\n",
    "But it does not leave you with much .   1\n",
    "You could hate it for the same reason . 1\n",
    "The performances are an absolute joy .  4\n",
    "```\n",
    "\n",
    "读取之后的DataSet具有以下的field\n",
    "\n",
    "| raw_words                               | target |\n",
    "| --------------------------------------- | ------ |\n",
    "| But it does not leave you with much .   | 1      |\n",
    "| You could hate it for the same reason . | 1      |\n",
    "| The performances are an absolute joy .  | 4      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**JsonLoader** 读取Json类型的数据集文件，数据必须按行存储，每行是一个包含各类属性的Json对象。例子如下\n",
    "\n",
    "```python\n",
    "from fastNLP.io.loader import JsonLoader\n",
    "loader = JsonLoader(\n",
    "    fields={'sentence1': 'raw_words1', 'sentence2': 'raw_words2', 'gold_label': 'target'}\n",
    ")\n",
    "```\n",
    "\n",
    "表示将Json对象中'sentence1'、'sentence2'和'gold_label'对应的值赋给'raw_words1'、'raw_words2'、'target'这三个fields\n",
    "\n",
    "```python\n",
    "data_set = loader._load('path/to/your/file')\n",
    "```\n",
    "\n",
    "数据集内容样例如下\n",
    "```\n",
    "{\"annotator_labels\": [\"neutral\"], \"captionID\": \"3416050480.jpg#4\", \"gold_label\": \"neutral\", ... }\n",
    "{\"annotator_labels\": [\"contradiction\"], \"captionID\": \"3416050480.jpg#4\", \"gold_label\": \"contradiction\", ... }\n",
    "{\"annotator_labels\": [\"entailment\"], \"captionID\": \"3416050480.jpg#4\", \"gold_label\": \"entailment\", ... }\n",
    "```\n",
    "\n",
    "读取之后的DataSet具有以下的field\n",
    "\n",
    "| raw_words0                                             | raw_words1                                        | target        |\n",
    "| ------------------------------------------------------ | ------------------------------------------------- | ------------- |\n",
    "| A person on a horse jumps over a broken down airplane. | A person is training his horse for a competition. | neutral       |\n",
    "| A person on a horse jumps over a broken down airplane. | A person is at a diner, ordering an omelette.     | contradiction |\n",
    "| A person on a horse jumps over a broken down airplane. | A person is outdoors, on a horse.                 | entailment    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastNLP.io.loader import CSVLoader\n",
    "data_set_loader = CSVLoader(\n",
    "    headers=('raw_words', 'target'), sep='\\t'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
